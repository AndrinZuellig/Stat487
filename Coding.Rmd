---
title: "R Notebook"
output: html_notebook
---

```{r}
library(MASS) # For LDA and QDA
library(caret) # For Confusion Matrix
library(ggplot2)
library(class)
library(tidyverse)
library(gridExtra)
library(ggpubr) # For plotting histograms with common legend
library(pROC) 
```


# Rylan EDA

```{r}


admission <- read.csv("admission.csv")


str(admission)
admission$De <- as.factor(admission$De)

# Split into training and testing sets
admission_test <- admission %>% 
  group_by(De) %>% 
  slice_head(n=5)

admission_train <- anti_join(admission, admission_test)

summary(admission_train)

# Scatter plot of GMAT vs GPA
ggplot(admission_train, aes(x=GPA, y=GMAT, col=De)) +
  geom_point() + 
  scale_color_manual("Admission Status",values=c("green", 'orange', "red"),
                    labels=c("Admit", "Borderline", "Do Not Admit")) +
  xlab("GPA") +
  ylab("GMAT Score") +
  theme_bw()

# Violin plots: GPA and GMAT vs admission status 
create_violin_plot <- function(data, y_var, y_label=y_var) {
  ggplot(data, aes(x = De, y = .data[[y_var]], fill = De)) +
    geom_violin() +
    scale_fill_manual(values = c("green", "orange", "red")) +
    guides(fill = "none") +
    xlab("Admission Status") +
    ylab(y_label) +
    scale_x_discrete(labels = c("Admit", "Borderline", "Do Not Admit")) +
    theme_bw()
}

gmat_vs_status <- create_violin_plot(admission_train, "GMAT", "GMAT Score")
gpa_vs_status <- create_violin_plot(admission_train, "GPA")
grid.arrange(gmat_vs_status, gpa_vs_status, nrow = 1, ncol = 2)

# Density plots
create_density <- function(data, xvar, xlabel = xvar) {
  p <- ggplot(data, aes(x = .data[[xvar]], fill = De)) +
    geom_density(alpha=0.7) +
    scale_fill_manual(
      name = "Admission Status",
      values = c("green", "orange", "red"), 
      labels = c("Admit", "Borderline", "Do Not Admit")
    ) +
    xlab(xlabel) +
    ylab("Density") +
    theme_bw() 
}

# Only need one y-label and one legend
gpa_density <- create_density(admission_train, "GPA") + theme(axis.title.y=element_blank())
gmat_density <- create_density(admission_train, "GMAT", "GMAT Score")

ggarrange(gmat_density, gpa_density, ncol=2, nrow=1, 
          common.legend = TRUE, legend="bottom")
```





```{r}
print(names(train_preds))
```




#Jocelyn #sorry i am not finished yet #please change anything you see wrong or can be improved, thanks

```{r}
#b)





#Linear Discriminant Analysis on Training Data
vector <- c(-c(28,29,30,31,56,57,58,59,82,83,84,85))
#eliminates last 4 observations of each of the 3 groups
lda_model <- lda(De ~ GMAT + GPA, data=admission, subset = vector)
#LDA with training data

lda_test_pred <- predict(lda_model, admission_test)
lda_test_class <- lda_test_pred$class
De_test <-admission_test$De
table(lda_test_class, De_test)
#LDA Confusion Table for Test Data

Fraction_correct_test_predictions <- mean(lda_test_class == De_test)
misclassification_rate_test_LDA <- 1 - Fraction_correct_test_predictions
misclassification_rate_test_LDA
#Overall LDA misclassification rate for Test Data

lda_train_pred <- predict(lda_model, admission_train)
lda_train_class <- lda_train_pred$class
De_train <- admission_train$De
table(lda_train_class, De_train)
#LDA Confusion Table for Training Data

Fraction_correct_train_predictions <- mean(lda_train_class == De_train)
misclassification_rate_train_LDA <- 1 - Fraction_correct_train_predictions
misclassification_rate_train_LDA
#Overall LDA misclassification rate for Training Data


##### plot LDA desicion boundary line



#c)

#Quadratic Discriminant Analysis on Training data
qda_model <- qda(De ~ GMAT + GPA, data=admission, subset = vector)
#QDA with training data

qda_test_pred <- predict(qda_model, admission_test)
qda_test_class <- qda_test_pred$class
table(qda_test_class, De_test) 
#QDA Confusion Table for Test Data

Fraction_correct_test_predictions_QDA <- mean(qda_test_class == De_test)
misclassification_rate_test_QDA <- 1 - Fraction_correct_test_predictions_QDA
misclassification_rate_test_QDA
#Overall QDA misclassification rate for Test Data

qda_train_pred <- predict(qda_model, admission_train)
qda_train_class <- qda_train_pred$class
table(qda_train_class, De_train)
#QDA Confusion Table for Training Data

Fraction_correct_train_predictions_QDA <- mean(qda_train_class == De_train)
misclassification_rate_train_QDA <- 1 - Fraction_correct_train_predictions_QDA
misclassification_rate_train_QDA
#Overall QDA misclassification rate for Training Data


##### plot QDA desicion boundary line

```


# Kira lda and qda (I commented out my code because Jocelyn has similar code, and I don't want it to cause any problems when running the code)
```{r}
# # lda with test data
# lda.fit <- lda(De ~ GPA + GMAT, data = admission_train)
# 
# lda.pred_test <- predict(lda.fit, admission_test)
# lda.class_test <- lda.pred_test$class
# 
# # confusion matrix
# conf_matrix_ldatest <- confusionMatrix(admission_test$De, lda.class_test)
# conf_matrix_ldatest
# 
# # str(conf_matrix_ldatest) look at this to do some of the steps below
# 
# #extracting sensitivities, specificities, and overall accuracy
# sensitivities_ldatest <- conf_matrix_ldatest$byClass[,"Sensitivity"]
# specificities_ldatest <- conf_matrix_ldatest$byClass[,"Specificity"]
# 
# overall_accuracy_ldatest <- conf_matrix_ldatest$overall["Accuracy"]
# 
# # the overall error rate is 1 - overall accuracy
# overall_error_rate_ldatest <- 1 - overall_accuracy_ldatest
# 
# cat("sensitivity for each class:", sensitivities_ldatest, "\n")
# cat("specificity for each class:", specificities_ldatest, "\n")
# 
# cat("avg of the sensitivities:", mean(sensitivities_ldatest), "\n")
# cat("avg of the specificities:", mean(specificities_ldatest), "\n")
# cat("overall error rate:", overall_error_rate_ldatest)
```


```{r}
# # lda with training data
# lda.pred_train <- predict(lda.fit, admission_train)
# lda.class_train <- lda.pred_train$class
# 
# # confusion matrix
# conf_matrix_ldatrain <- confusionMatrix(admission_train$De, lda.class_train)
# conf_matrix_ldatrain
# 
# # extracting sensitivities, specificities, and overall accuracy
# sensitivities_ldatrain <- conf_matrix_ldatrain$byClass[,"Sensitivity"]
# specificities_ldatrain <- conf_matrix_ldatrain$byClass[,"Specificity"]
# 
# overall_accuracy_ldatrain <- conf_matrix_ldatrain$overall["Accuracy"]
# 
# # the overall error rate is 1 - overall accuracy
# overall_error_rate_ldatrain <- 1 - overall_accuracy_ldatrain
# 
# cat("sensitivity for each class:", sensitivities_ldatrain, "\n")
# cat("specificity for each class:", specificities_ldatrain, "\n")
# 
# cat("avg of the sensitivities:", mean(sensitivities_ldatrain), "\n")
# cat("avg of the specificities:", mean(specificities_ldatrain), "\n")
# cat("overall error rate:", overall_error_rate_ldatrain)
```


```{r}
# # qda with test data
# qda.fit <- qda(De ~ GPA + GMAT, data = admission_train)
# 
# qda.pred_test <- predict(qda.fit, admission_test)
# qda.class_test <- qda.pred_test$class
# 
# # confusion matrix
# conf_matrix_qdatest <- confusionMatrix(admission_test$De, qda.class_test)
# conf_matrix_qdatest
# 
# # str(conf_matrix_qdatest) look at this to do some of the steps below
# 
# # extracting sensitivities, specificities, and overall accuracy
# sensitivities_qdatest <- conf_matrix_qdatest$byClass[,"Sensitivity"]
# specificities_qdatest <- conf_matrix_qdatest$byClass[,"Specificity"]
# 
# overall_accuracy_qdatest <- conf_matrix_qdatest$overall["Accuracy"]
# 
# # the overall error rate is 1 - overall accuracy
# overall_error_rate_qdatest <- 1 - overall_accuracy_qdatest
# 
# cat("sensitivity for each class:", sensitivities_qdatest, "\n")
# cat("specificity for each class:", specificities_qdatest, "\n")
# 
# cat("avg of the sensitivities:", mean(sensitivities_qdatest), "\n")
# cat("avg of the specificities:", mean(specificities_qdatest), "\n")
# cat("overall error rate:", overall_error_rate_qdatest)
```


```{r}
# # qda with training data
# qda.pred_train <- predict(qda.fit, admission_train)
# qda.class_train <- qda.pred_train$class
# 
# # confusion matrix
# conf_matrix_qdatrain <- confusionMatrix(admission_train$De, qda.class_train)
# conf_matrix_qdatrain
# 
# # extracting sensitivities, specificities, and overall accuracy
# sensitivities_qdatrain <- conf_matrix_qdatrain$byClass[,"Sensitivity"]
# specificities_qdatrain <- conf_matrix_qdatrain$byClass[,"Specificity"]
# 
# overall_accuracy_qdatrain <- conf_matrix_qdatrain$overall["Accuracy"]
# 
# # the overall error rate is 1 - overall accuracy
# overall_error_rate_qdatrain <- 1 - overall_accuracy_qdatrain
# 
# cat("sensitivity for each class:", sensitivities_qdatrain, "\n")
# cat("specificity for each class:", specificities_qdatrain, "\n")
# 
# cat("avg of the sensitivities:", mean(sensitivities_qdatrain), "\n")
# cat("avg of the specificities:", mean(specificities_qdatrain), "\n")
# cat("overall error rate:", overall_error_rate_qdatrain)
```


# Rylan KNN

```{r}
set.seed(1)

train_X <- as.matrix(admission_train[, c('GPA', 'GMAT')])
train_X <- scale(train_X)
train_Y <- admission_train$De

# When scaling test set, use parameters (mean and sd) from train set
train_means <- attr(train_X, "scaled:center")
train_sds <- attr(train_X, "scaled:scale")
test_X <- as.matrix(admission_test[, c('GPA', 'GMAT')])
test_X <- scale(test_X, center = train_means, scale = train_sds)
test_Y <- admission_test$De


calc_knn_error <- function(k, is_train_data) {
  if(is_train_data) {
    knn_train_out <- knn(train_X, train_X, train_Y, k=k, prob=T)
    return(1 - mean(train_Y == knn_train_out))
  }
  else {
    knn_test_out <- knn(train_X, test_X, train_Y, k=k)
    return(1 - mean(test_Y == knn_test_out))
  }
}

test_error_rates <- sapply(1:nrow(admission_train), calc_knn_error, 
                           is_train_data=F)
plot(test_error_rates)

optimal_k <- which.min(test_error_rates)

# Train predictions
knn_train_out <- knn(train_X, train_X, train_Y, k=optimal_k, prob=T)

# Automate extraction of performance metrics and confusion matrix
get_perf_metrics <- function(pred, ground_truth){
  cm <- table(True_Status=ground_truth, Predicted_Status=pred)
  # Use this to get sensitivities and specificities
  cm <- confusionMatrix(cm, positive="1")
  sensitivities <- cm$byClass[,"Sensitivity"]
  specificities <- cm$byClass[,"Specificity"]
  overall_accuracy <- cm$overall["Accuracy"]
  overall_error_rate <- 1 - overall_accuracy
  mean_sens <- mean(sensitivities)
  mean_spec <- mean(specificities)
  
  # Calculate the AUC
  probs <- as.numeric(attributes(pred)$prob)
  auc <- multiclass.roc(ground_truth, probs)$auc
  
  # Put the results into a table with labeled column names
  result_matrix <- matrix(c(sensitivities, mean_sens, 
                            specificities, mean_spec, 
                            overall_accuracy, overall_error_rate, 
                            auc), 
                          nrow = 1, byrow = TRUE)
  
  colnames(result_matrix) <- c(paste0(names(sensitivities), " Sensitivity"),
                               "Mean Sensitivity",
                               paste0(names(specificities), " Specificity"),
                               "Mean Specificity",
                               "Overall Accuracy", 
                               "Overall Error Rate", 
                               "AUC")
  
  # Return confusion matrix and results table as list
  results <- list(Confusion_Matrix=cm$table, Metrics=result_matrix)
  return(results)
}

# Train Performance Metrics 
train_metrics <- get_perf_metrics(knn_train_out, train_Y)
train_metrics

# Test Predictions
knn_test_out <- knn(train_X, test_X, train_Y, k=optimal_k, prob=T)

# Test Performance Metrics
test_metrics <- get_perf_metrics(knn_test_out, test_Y)
test_metrics

```






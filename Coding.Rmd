---
title: "R Notebook"
output: html_notebook
---

```{r}
library(MASS) # For LDA and QDA
library(caret) # For Confusion Matrix
library(ggplot2)
library(class)
library(tidyverse)
library(gridExtra)
library(ggpubr) # For plotting histograms with common legend
library(pROC) 
```

# Rylan EDA

```{r}


admission <- read.csv("admission.csv")


str(admission)
admission$De <- as.factor(admission$De)

# Split into training and testing sets
admission_test <- admission %>% 
  group_by(De) %>% 
  slice_head(n=5)

admission_train <- anti_join(admission, admission_test)

summary(admission_train)

# Scatter plot of GMAT vs GPA
ggplot(admission_train, aes(x=GPA, y=GMAT, col=De)) +
  geom_point() + 
  scale_color_manual("Admission Status",values=c("green", 'orange', "red"),
                    labels=c("Admit", "Borderline", "Do Not Admit")) +
  xlab("GPA") +
  ylab("GMAT Score") +
  theme_bw()

# Violin plots: GPA and GMAT vs admission status 
create_violin_plot <- function(data, y_var, y_label=y_var) {
  ggplot(data, aes(x = De, y = .data[[y_var]], fill = De)) +
    geom_violin() +
    scale_fill_manual(values = c("green", "orange", "red")) +
    guides(fill = "none") +
    xlab("Admission Status") +
    ylab(y_label) +
    scale_x_discrete(labels = c("Admit", "Borderline", "Do Not Admit")) +
    theme_bw()
}

gmat_vs_status <- create_violin_plot(admission_train, "GMAT", "GMAT Score")
gpa_vs_status <- create_violin_plot(admission_train, "GPA")
grid.arrange(gmat_vs_status, gpa_vs_status, nrow = 1, ncol = 2)

# Density plots
create_density <- function(data, xvar, xlabel = xvar) {
  p <- ggplot(data, aes(x = .data[[xvar]], fill = De)) +
    geom_density(alpha=0.7) +
    scale_fill_manual(
      name = "Admission Status",
      values = c("green", "orange", "red"), 
      labels = c("Admit", "Borderline", "Do Not Admit")
    ) +
    xlab(xlabel) +
    ylab("Density") +
    theme_bw() 
}

# Only need one y-label and one legend
gpa_density <- create_density(admission_train, "GPA") + theme(axis.title.y=element_blank())
gmat_density <- create_density(admission_train, "GMAT", "GMAT Score")

ggarrange(gmat_density, gpa_density, ncol=2, nrow=1, 
          common.legend = TRUE, legend="bottom")
```

```{r}
print(names(train_preds))
```

#Jocelyn # lda decision boundaries, lda confusion matrices # qda decision boundaries, qda confusion matrices

```{r}
#b)

#Linear Discriminant Analysis on Training Data
lda_model <- lda(De ~ GPA + GMAT, data=admission_train)
#LDA with training data

lda_test_pred <- predict(lda_model, admission_test)
lda_test_class <- lda_test_pred$class
De_test <-admission_test$De
table(lda_test_class, De_test)
#LDA Confusion Table for Test Data

Fraction_correct_test_predictions <- mean(lda_test_class == De_test)
misclassification_rate_test_LDA <- 1 - Fraction_correct_test_predictions
misclassification_rate_test_LDA
#Overall LDA misclassification rate for Test Data

lda_train_pred <- predict(lda_model, admission_train)
lda_train_class <- lda_train_pred$class
De_train <- admission_train$De
table(lda_train_class, De_train)
#LDA Confusion Table for Training Data

Fraction_correct_train_predictions <- mean(lda_train_class == De_train)
misclassification_rate_train_LDA <- 1 - Fraction_correct_train_predictions
misclassification_rate_train_LDA
#Overall LDA misclassification rate for Training Data



##### plot LDA desicion boundary line
GPA_range <- seq(min(admission_train$GPA) - 1, max(admission_train$GPA) + 1, length.out = 500)
GMAT_range <- seq(min(admission_train$GMAT) - 1, max(admission_train$GMAT) + 1, length.out = 500)
grid <- expand.grid(GPA = GPA_range, GMAT = GMAT_range)

# Predict the class probabilities for the grid points
grid$De <- predict(lda_model, newdata = grid)$class

# Create numeric labels for the class (1 = admit, 2 = notadmit, 3 = border)
grid$De_num <- as.numeric(factor(grid$De, levels = c("admit", "notadmit", "border")))


#LDA desicion boundary line on training data
lda_plot_train <- ggplot(data = admission_train, aes(x = GPA, y = GMAT, 
                                                     color = De)) +
  geom_point() +
  # geom_raster for smooth background # alpha = 0.1 for transparent background
  geom_raster(data = grid, aes(x = GPA, y = GMAT, fill = De), alpha = 0.1) +  
  # Point colors
  scale_color_manual("Admission Status", 
                     values = c("admit" = "green",
                                "notadmit" = "red", "border" = "orange"),
                     labels = c("admit" = "Admit","border" = "Borderline",
                                "notadmit" = "Do Not Admit")) +
  # Background colors
  scale_fill_manual("Classification",
                    values = c("admit" = "green",
                               "notadmit" = "red", "border" = "orange"), 
                    labels = c("admit" = "Admit", "border" = "Borderline",
                               "notadmit" = "Do Not Admit")) +
  theme_minimal() +
  # Remove grid lines
  theme(panel.grid = element_blank(), 
        # Center the title
        plot.title = element_text(hjust = 0.5)) +  
  labs(title = "LDA Decision Boundary Lines on Training Data") +
  # Add decision boundary lines
  geom_contour(data = grid, aes(x = GPA, y = GMAT, z = De_num), 
               color = "black", size = 0.2)
  

#LDA desicion boundary line on test data
lda_plot_test <- ggplot(data = admission_test, aes(x = GPA, y = GMAT, 
                                                   color = De)) +
  geom_point() +
  # geom_raster for smooth background # alpha = 0.1 for transparent background
  geom_raster(data = grid, aes(x = GPA, y = GMAT, fill = De), alpha = 0.1) +  
  # Point colors
  scale_color_manual("Admission Status", 
                     values = c("admit" = "green",
                                "notadmit" = "red", "border" = "orange"),
                     labels = c("admit" = "Admit", "border" = "Borderline",
                                "notadmit" = "Do Not Admit")) +
  # Background colors
  scale_fill_manual("Classification",
                    values = c("admit" = "green",
                               "notadmit" = "red", "border" = "orange"), 
                    labels = c("admit" = "Admit", "border" = "Borderline",
                               "notadmit" = "Do Not Admit")) +
  theme_minimal() +
  # Remove grid lines
  theme(panel.grid = element_blank(), 
        # Center the title
        plot.title = element_text(hjust = 0.5)) +  
  labs(title = "LDA Decision Boundary Lines on Test Data") +
  # Add decision boundary lines
  geom_contour(data = grid, aes(x = GPA, y = GMAT, z = De_num), 
               color = "black", size = 0.2)


lda_plot_train
lda_plot_test




#c)

#Quadratic Discriminant Analysis on Training data
qda_model <- qda(De ~ GPA + GMAT, data=admission_train)
#QDA with training data

qda_test_pred <- predict(qda_model, admission_test)
qda_test_class <- qda_test_pred$class
table(qda_test_class, De_test) 
#QDA Confusion Table for Test Data

Fraction_correct_test_predictions_QDA <- mean(qda_test_class == De_test)
misclassification_rate_test_QDA <- 1 - Fraction_correct_test_predictions_QDA
misclassification_rate_test_QDA
#Overall QDA misclassification rate for Test Data

qda_train_pred <- predict(qda_model, admission_train)
qda_train_class <- qda_train_pred$class
table(qda_train_class, De_train)
#QDA Confusion Table for Training Data

Fraction_correct_train_predictions_QDA <- mean(qda_train_class == De_train)
misclassification_rate_train_QDA <- 1 - Fraction_correct_train_predictions_QDA
misclassification_rate_train_QDA
#Overall QDA misclassification rate for Training Data



##### plot QDA desicion boundary line
grid2 <- expand.grid(GPA = GPA_range, GMAT = GMAT_range)

# Predict the class probabilities for the grid points
grid2$De <- predict(qda_model, newdata = grid2)$class

# Create numeric labels for the class (1 = admit, 2 = notadmit, 3 = border)
grid2$De_num <- as.numeric(factor(grid2$De, levels = c("admit", "notadmit", "border")))


# QDA desicion boundary line on training data
qda_plot_train <- ggplot(data = admission_train, aes(x = GPA, y = GMAT,
                                                     color = De)) +
  geom_point() +
  # geom_raster for smooth background # alpha = 0.1 for transparent background
  geom_raster(data = grid2, aes(x = GPA, y = GMAT, fill = De), alpha = 0.1) +  
  # Point colors
  scale_color_manual("Admission Status", 
                     values = c("admit" = "green",
                                "notadmit" = "red", "border" = "orange"),
                     labels = c("admit" = "Admit", "border" = "Borderline",
                                "notadmit" = "Do Not Admit")) +
  # Background colors
  scale_fill_manual("Classification",
                    values = c("admit" = "green",
                               "notadmit" = "red", "border" = "orange"), 
                    labels = c("admit" = "Admit", "border" = "Borderline",
                               "notadmit" = "Do Not Admit")) +
  theme_minimal() +
  # Remove grid lines
  theme(panel.grid = element_blank(), 
        # Center the title
        plot.title = element_text(hjust = 0.5)) +  
  labs(title = "QDA Decision Boundary Lines on Training Data") +
  # Add decision boundary lines
  geom_contour(data = grid2, aes(x = GPA, y = GMAT, z = De_num), 
               color = "black", size = 0.2)


# QDA desicion boundary line on test data
qda_plot_test <- ggplot(data = admission_test, aes(x = GPA, y = GMAT,
                                                   color = De)) +
  geom_point() +
  # geom_raster for smooth background # alpha = 0.1 for transparent background
  geom_raster(data = grid2, aes(x = GPA, y = GMAT, fill = De), alpha = 0.1) +  
  # Point colors
  scale_color_manual("Admission Status", 
                     values = c("admit" = "green",
                                "notadmit" = "red", "border" = "orange"),
                     labels = c("admit" = "Admit", "border" = "Borderline",
                                "notadmit" = "Do Not Admit")) +
  # Background colors
  scale_fill_manual("Classification",
                    values = c("admit" = "green",
                               "notadmit" = "red", "border" = "orange"), 
                    labels = c("admit" = "Admit", "border" = "Borderline",
                               "notadmit" = "Do Not Admit")) +
  theme_minimal() +
  # Remove grid lines
  theme(panel.grid = element_blank(), 
        # Center the title
        plot.title = element_text(hjust = 0.5)) +  
  labs(title = "QDA Decision Boundary Lines on Test Data") +
  # Add decision boundary lines
  geom_contour(data = grid2, aes(x = GPA, y = GMAT, z = De_num), 
               color = "black", size = 0.2)


qda_plot_train
qda_plot_test



```

# Kira lda and qda (I commented out my code because Jocelyn has similar code, and I don't want it to cause any problems when running the code)

```{r}
# # lda with test data
# lda.fit <- lda(De ~ GPA + GMAT, data = admission_train)
# 
# lda.pred_test <- predict(lda.fit, admission_test)
# lda.class_test <- lda.pred_test$class
# 
# # confusion matrix
# conf_matrix_ldatest <- confusionMatrix(admission_test$De, lda.class_test)
# conf_matrix_ldatest
# 
# # str(conf_matrix_ldatest) look at this to do some of the steps below
# 
# #extracting sensitivities, specificities, and overall accuracy
# sensitivities_ldatest <- conf_matrix_ldatest$byClass[,"Sensitivity"]
# specificities_ldatest <- conf_matrix_ldatest$byClass[,"Specificity"]
# 
# overall_accuracy_ldatest <- conf_matrix_ldatest$overall["Accuracy"]
# 
# # the overall error rate is 1 - overall accuracy
# overall_error_rate_ldatest <- 1 - overall_accuracy_ldatest
# 
# cat("sensitivity for each class:", sensitivities_ldatest, "\n")
# cat("specificity for each class:", specificities_ldatest, "\n")
# 
# cat("avg of the sensitivities:", mean(sensitivities_ldatest), "\n")
# cat("avg of the specificities:", mean(specificities_ldatest), "\n")
# cat("overall error rate:", overall_error_rate_ldatest)
```

```{r}
# # lda with training data
# lda.pred_train <- predict(lda.fit, admission_train)
# lda.class_train <- lda.pred_train$class
# 
# # confusion matrix
# conf_matrix_ldatrain <- confusionMatrix(admission_train$De, lda.class_train)
# conf_matrix_ldatrain
# 
# # extracting sensitivities, specificities, and overall accuracy
# sensitivities_ldatrain <- conf_matrix_ldatrain$byClass[,"Sensitivity"]
# specificities_ldatrain <- conf_matrix_ldatrain$byClass[,"Specificity"]
# 
# overall_accuracy_ldatrain <- conf_matrix_ldatrain$overall["Accuracy"]
# 
# # the overall error rate is 1 - overall accuracy
# overall_error_rate_ldatrain <- 1 - overall_accuracy_ldatrain
# 
# cat("sensitivity for each class:", sensitivities_ldatrain, "\n")
# cat("specificity for each class:", specificities_ldatrain, "\n")
# 
# cat("avg of the sensitivities:", mean(sensitivities_ldatrain), "\n")
# cat("avg of the specificities:", mean(specificities_ldatrain), "\n")
# cat("overall error rate:", overall_error_rate_ldatrain)
```

```{r}
# # qda with test data
# qda.fit <- qda(De ~ GPA + GMAT, data = admission_train)
# 
# qda.pred_test <- predict(qda.fit, admission_test)
# qda.class_test <- qda.pred_test$class
# 
# # confusion matrix
# conf_matrix_qdatest <- confusionMatrix(admission_test$De, qda.class_test)
# conf_matrix_qdatest
# 
# # str(conf_matrix_qdatest) look at this to do some of the steps below
# 
# # extracting sensitivities, specificities, and overall accuracy
# sensitivities_qdatest <- conf_matrix_qdatest$byClass[,"Sensitivity"]
# specificities_qdatest <- conf_matrix_qdatest$byClass[,"Specificity"]
# 
# overall_accuracy_qdatest <- conf_matrix_qdatest$overall["Accuracy"]
# 
# # the overall error rate is 1 - overall accuracy
# overall_error_rate_qdatest <- 1 - overall_accuracy_qdatest
# 
# cat("sensitivity for each class:", sensitivities_qdatest, "\n")
# cat("specificity for each class:", specificities_qdatest, "\n")
# 
# cat("avg of the sensitivities:", mean(sensitivities_qdatest), "\n")
# cat("avg of the specificities:", mean(specificities_qdatest), "\n")
# cat("overall error rate:", overall_error_rate_qdatest)
```

```{r}
# # qda with training data
# qda.pred_train <- predict(qda.fit, admission_train)
# qda.class_train <- qda.pred_train$class
# 
# # confusion matrix
# conf_matrix_qdatrain <- confusionMatrix(admission_train$De, qda.class_train)
# conf_matrix_qdatrain
# 
# # extracting sensitivities, specificities, and overall accuracy
# sensitivities_qdatrain <- conf_matrix_qdatrain$byClass[,"Sensitivity"]
# specificities_qdatrain <- conf_matrix_qdatrain$byClass[,"Specificity"]
# 
# overall_accuracy_qdatrain <- conf_matrix_qdatrain$overall["Accuracy"]
# 
# # the overall error rate is 1 - overall accuracy
# overall_error_rate_qdatrain <- 1 - overall_accuracy_qdatrain
# 
# cat("sensitivity for each class:", sensitivities_qdatrain, "\n")
# cat("specificity for each class:", specificities_qdatrain, "\n")
# 
# cat("avg of the sensitivities:", mean(sensitivities_qdatrain), "\n")
# cat("avg of the specificities:", mean(specificities_qdatrain), "\n")
# cat("overall error rate:", overall_error_rate_qdatrain)
```

# Rylan KNN

```{r}
set.seed(70)

admission <- read.csv("admission.csv")
admission$De <- as.factor(admission$De)

admission_train <- anti_join(admission, admission_test)

train_X <- as.matrix(admission_train[, c('GPA', 'GMAT')])
train_X <- scale(train_X)
train_Y <- admission_train$De

test_X <- as.matrix(admission_test[, c('GPA', 'GMAT')])
test_X <- scale(test_X)
test_Y <- admission_test$De

# Calculates error and predictions (can use on either train
# or test data, in case we want to plot train and test error rates together)
calc_knn_error <- function(num_neighbors, train_obs, test_obs, train_labels, 
                           test_labels){
  knn_model <- knn3(train_obs, train_labels, k = num_neighbors)
  knn_pred <- predict(knn_model, test_obs, type = "class")  
  err <- 1 - mean(test_labels == knn_pred)
  return(err)
}

test_error_rates <- numeric(nrow(admission_train))
for (i in 1:nrow(admission_train)) {
  test_error_rates[i] <- calc_knn_error(num_neighbors=i, train_obs = train_X, 
                           test_obs=test_X, train_labels=train_Y, 
                           test_labels=test_Y)
}

plot(test_error_rates)
optimal_k <- which.min(test_error_rates)

# Train k-NN model with optimal k
knn_model <- knn3(train_X, train_Y, k = optimal_k)

# Get predicted class probabilities
knn_train_probs <- predict(knn_model, train_X, type = "prob")
knn_test_probs <- predict(knn_model, test_X, type = "prob")

# Get predicted class labels
knn_train_out <- predict(knn_model, train_X, type = "class")
knn_test_out <- predict(knn_model, test_X, type = "class")

# Automate extraction of performance metrics and confusion matrix
get_perf_metrics <- function(probs, pred, ground_truth){
  cm <- table(True_Status=ground_truth, Predicted_Status=pred)
  # Use this to get sensitivities and specificities
  cm <- confusionMatrix(cm, positive="1")
  sensitivities <- cm$byClass[,"Sensitivity"]
  specificities <- cm$byClass[,"Specificity"]
  overall_accuracy <- cm$overall["Accuracy"]
  overall_error_rate <- 1 - overall_accuracy
  mean_sens <- mean(sensitivities)
  mean_spec <- mean(specificities)
  
  # Calculate the AUC
  auc <- multiclass.roc(ground_truth, probs)$auc
  
  # Put the results into a table
  result_vector <- c(sensitivities, mean_sens, 
                     specificities, mean_spec, 
                     overall_accuracy, overall_error_rate, 
                     auc)
  
  row_names <- c(paste0(names(sensitivities), " Sensitivity"),
                 "Mean Sensitivity",
                 paste0(names(specificities), " Specificity"),
                 "Mean Specificity",
                 "Overall Accuracy", 
                 "Overall Error Rate", 
                 "AUC")
  
  result_df <- data.frame(Metric = row_names, Value = result_vector)
  
  # Return confusion matrix and results table as list
  results <- list(Confusion_Matrix=cm$table, Metrics=result_df)
  return(results)
}

# Train Performance Metrics 
train_metrics <- get_perf_metrics(knn_train_probs, knn_train_out, train_Y)
train_metrics

# Test Performance Metrics
test_metrics <- get_perf_metrics(knn_test_probs, knn_test_out, test_Y)
test_metrics


combined <- train_metrics$Metrics %>% inner_join(test_metrics$Metric, by = "Metric") %>%
  rename("Train" = Value.x, "Test" = Value.y) 
View(combined)


```









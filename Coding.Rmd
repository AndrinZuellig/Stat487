---
title: "R Notebook"
output: html_notebook
---

this is a test

```{r}
library(tidyverse)
```

# Rylan EDA

```{r}
library(tidyverse)
library(gridExtra)
library(ggpubr) # For plotting histograms with common legend

admission <- read.csv("admission.csv")

str(admission)
admission$De <- as.factor(admission$De)

# Split into training and testing sets
admission_test <- admission %>% 
  group_by(De) %>% 
  slice_tail(n=4)

admission_train <- anti_join(admission, admission_test)

summary(admission_train)

# Scatter plot of GMAT vs GPA
ggplot(admission_train, aes(x=GPA, y=GMAT, col=De)) +
  geom_point() + 
  scale_color_manual("Admission Status",values=c("green", 'orange', "red"),
                    labels=c("Admit", "Borderline", "Do Not Admit")) +
  xlab("GPA") +
  ylab("GMAT Score") +
  theme_bw()

# Violin plots: GPA and GMAT vs admission status 
create_violin_plot <- function(data, y_var, y_label=y_var) {
  ggplot(data, aes(x = De, y = .data[[y_var]], fill = De)) +
    geom_violin() +
    scale_fill_manual(values = c("green", "orange", "red")) +
    guides(fill = "none") +
    xlab("Admission Status") +
    ylab(y_label) +
    scale_x_discrete(labels = c("Admit", "Borderline", "Do Not Admit")) +
    theme_bw()
}

gmat_vs_status <- create_violin_plot(admission_train, "GMAT", "GMAT Score")
gpa_vs_status <- create_violin_plot(admission_train, "GPA")
grid.arrange(gmat_vs_status, gpa_vs_status, nrow = 1, ncol = 2)

# Density plots
create_density <- function(data, xvar, xlabel = xvar) {
  p <- ggplot(data, aes(x = .data[[xvar]], fill = De)) +
    geom_density(alpha=0.7) +
    scale_fill_manual(
      name = "Admission Status",
      values = c("green", "orange", "red"), 
      labels = c("Admit", "Borderline", "Do Not Admit")
    ) +
    xlab(xlabel) +
    ylab("Density") +
    theme_bw() 
}

# Only need one y-label and one legend
gpa_density <- create_density(admission_train, "GPA") + theme(axis.title.y=element_blank())
gmat_density <- create_density(admission_train, "GMAT", "GMAT Score")

ggarrange(gmat_density, gpa_density, ncol=2, nrow=1, 
          common.legend = TRUE, legend="bottom")
```

#Jocelyn #sorry i am not finished yet #please change anything you see wrong or can be improved, thanks

```{r}
#b)

library(MASS) # For LDA and QDA
library(caret) # For Confusion Matrix
library(ggplot2)



#Linear Discriminant Analysis on Training Data
vector <- c(-c(28,29,30,31,56,57,58,59,82,83,84,85))
#eliminates last 4 observations of each of the 3 groups
lda_model <- lda(De ~ GMAT + GPA, data=admission, subset = vector)
#LDA with training data

lda_test_pred <- predict(lda_model, admission_test)
lda_test_class <- lda_test_pred$class
De_test <-admission_test$De
table(lda_test_class, De_test)
#LDA Confusion Table for Test Data

Fraction_correct_test_predictions <- mean(lda_test_class == De_test)
misclassification_rate_test_LDA <- 1 - Fraction_correct_test_predictions
misclassification_rate_test_LDA
#Overall LDA misclassification rate for Test Data

lda_train_pred <- predict(lda_model, admission_train)
lda_train_class <- lda_train_pred$class
De_train <- admission_train$De
table(lda_train_class, De_train)
#LDA Confusion Table for Training Data

Fraction_correct_train_predictions <- mean(lda_train_class == De_train)
misclassification_rate_train_LDA <- 1 - Fraction_correct_train_predictions
misclassification_rate_train_LDA
#Overall LDA misclassification rate for Training Data


##### plot LDA desicion boundary line



#c)

#Quadratic Discriminant Analysis on Training data
qda_model <- qda(De ~ GMAT + GPA, data=admission, subset = vector)
#QDA with training data

qda_test_pred <- predict(qda_model, admission_test)
qda_test_class <- qda_test_pred$class
table(qda_test_class, De_test) 
#QDA Confusion Table for Test Data

Fraction_correct_test_predictions_QDA <- mean(qda_test_class == De_test)
misclassification_rate_test_QDA <- 1 - Fraction_correct_test_predictions_QDA
misclassification_rate_test_QDA
#Overall QDA misclassification rate for Test Data

qda_train_pred <- predict(qda_model, admission_train)
qda_train_class <- qda_train_pred$class
table(qda_train_class, De_train)
#QDA Confusion Table for Training Data

Fraction_correct_train_predictions_QDA <- mean(qda_train_class == De_train)
misclassification_rate_train_QDA <- 1 - Fraction_correct_train_predictions_QDA
misclassification_rate_train_QDA
#Overall QDA misclassification rate for Training Data


##### plot QDA desicion boundary line

```

# Rylan KNN

```{r}
set.seed(1)

train_X <- as.matrix(admission_train[, c('GPA', 'GMAT')])
train_X <- scale(train_X)
train_Y <- admission_train$De

# When scaling test set, use parameters (mean and sd) from train set
train_means <- attr(train_X, "scaled:center")
train_sds <- attr(train_X, "scaled:scale")
test_X <- as.matrix(admission_test[, c('GPA', 'GMAT')])
test_X <- scale(test_X, center = train_means, scale = train_sds)
test_Y <- admission_test$De


calc_knn_error <- function(k, is_train_data) {
  if(is_train_data) {
    knn_train_out <- knn(train_X, train_X, train_Y, k=k, prob=T)
    return(1 - mean(train_Y == knn_train_out))
  }
  else {
    knn_test_out <- knn(train_X, test_X, train_Y, k=k)
    return(1 - mean(test_Y == knn_test_out))
  }
}

test_error_rates <- sapply(1:nrow(admission_train), calc_knn_error, 
                           is_train_data=F)
plot(test_error_rates)

optimal_k <- which.min(test_error_rates)

# Train predictions
knn_train_out <- knn(train_X, train_X, train_Y, k=optimal_k, prob=T)

# Automate extraction of performance metrics and confusion matrix
get_perf_metrics <- function(pred, ground_truth){
  cm <- table(True_Status=ground_truth, Predicted_Status=pred)
  # Use this to get sensitivities and specificities
  cm <- confusionMatrix(cm, positive="1")
  sensitivities <- cm$byClass[,"Sensitivity"]
  specificities <- cm$byClass[,"Specificity"]
  overall_accuracy <- cm$overall["Accuracy"]
  overall_error_rate <- 1 - overall_accuracy
  mean_sens <- mean(sensitivities)
  mean_spec <- mean(specificities)
  
  # Calculate the AUC
  probs <- as.numeric(attributes(pred)$prob)
  auc <- multiclass.roc(ground_truth, probs)$auc
  
  # Put the results into a table with labeled column names
  result_matrix <- matrix(c(sensitivities, mean_sens, 
                            specificities, mean_spec, 
                            overall_accuracy, overall_error_rate, 
                            auc), 
                          nrow = 1, byrow = TRUE)
  
  colnames(result_matrix) <- c(paste0(names(sensitivities), " Sensitivity"),
                               "Mean Sensitivity",
                               paste0(names(specificities), " Specificity"),
                               "Mean Specificity",
                               "Overall Accuracy", 
                               "Overall Error Rate", 
                               "AUC")
  
  # Return confusion matrix and results table as list
  results <- list(Confusion_Matrix=cm$table, Metrics=result_matrix)
  return(results)
}

# Train Performance Metrics 
train_metrics <- get_perf_metrics(knn_train_out, train_Y)
train_metrics

# Test Predictions
knn_test_out <- knn(train_X, test_X, train_Y, k=optimal_k, prob=T)

# Test Performance Metrics
test_metrics <- get_perf_metrics(knn_test_out, test_Y)
test_metrics

```
